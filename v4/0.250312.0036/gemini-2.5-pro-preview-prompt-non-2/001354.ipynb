{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6bf4b4",
   "metadata": {},
   "source": [
    "# Exploring Dandiset 001354: Hippocampal neuronal responses to programmable antigen-gated G-protein-coupled engineered receptor activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ee273",
   "metadata": {},
   "source": [
    "**Important Note:** This notebook was AI-generated and has not been fully verified. Please be cautious when interpreting the code or results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d659c",
   "metadata": {},
   "source": [
    "## Overview of the Dandiset\n",
    "\n",
    "This Dandiset, [DANDI:001354 (version 0.250312.0036)](https://dandiarchive.org/dandiset/001354/0.250312.0036), contains single cell electrophysiological recordings of mouse hippocampal CA1 neurons. These recordings were made in response to activation of programmable antigen-gated G-protein-coupled engineered receptors (PAGERs). Neurons were transfected with an AAV1/2-hSyn-a-mCherry-PAGER-Gi-P2A-mEGFP, and responses were recorded during the application of DCZ (100 nM) or DCZ + soluble mCherry (1 uM).\n",
    "\n",
    "The study aims to understand neuronal responses under these specific chemogenetic manipulations.\n",
    "\n",
    "**Citation:** Klein, Peter (2025) Hippocampal neuronal responses to programmable antigen-gated G-protein-coupled engineered receptor activation (Version 0.250312.0036) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.001354/0.250312.0036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44098b",
   "metadata": {},
   "source": [
    "## What this notebook covers\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Listing required Python packages.\n",
    "2. Connecting to the DANDI archive and loading basic information about Dandiset 001354.\n",
    "3. Listing some of the assets (NWB files) available in this Dandiset.\n",
    "4. Loading one of the NWB files and exploring its basic metadata.\n",
    "5. Providing a link to explore the NWB file on Neurosift.\n",
    "6. Demonstrating how to load and visualize some example data from the NWB file.\n",
    "\n",
    "**Note:** The step to get detailed NWB file information using `tools_cli.py nwb-file-info` timed out during the generation of this notebook. Therefore, the sections on loading specific data paths from the NWB file will be more generic and might require adjustments based on the actual file structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337f40e",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "\n",
    "This notebook requires the following Python packages. It is assumed they are already installed on your system.\n",
    "\n",
    "* `dandi` (for interacting with the DANDI API)\n",
    "* `pynwb` (for working with NWB files)\n",
    "* `h5py` (as a backend for pynwb)\n",
    "* `numpy` (for numerical operations)\n",
    "* `matplotlib` (for plotting)\n",
    "* `itertools` (used in the example DANDI API code)\n",
    "* `requests` and `remfile` (often dependencies for streaming NWB data)\n",
    "* `seaborn` (for enhanced plotting styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aa054",
   "metadata": {},
   "source": [
    "## Loading the Dandiset using the DANDI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9efb2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:03.969568Z",
     "iopub.status.busy": "2025-05-09T19:35:03.969384Z",
     "iopub.status.idle": "2025-05-09T19:35:05.476514Z",
     "shell.execute_reply": "2025-05-09T19:35:05.475775Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pynwb import NWBHDF5IO\n",
    "import h5py # For direct HDF5 access if needed\n",
    "import requests # For fetching S3 URLs\n",
    "import tempfile # If downloading is chosen\n",
    "import os # For file operations\n",
    "\n",
    "# Apply seaborn styling for plots (except for images)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb34f67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:05.478227Z",
     "iopub.status.busy": "2025-05-09T19:35:05.477943Z",
     "iopub.status.idle": "2025-05-09T19:35:05.674795Z",
     "shell.execute_reply": "2025-05-09T19:35:05.674182Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dandiset name: Hippocampal neuronal responses to programmable antigen-gated G-protein-coupled engineered receptor activation\n",
      "Dandiset URL: https://dandiarchive.org/dandiset/001354/0.250312.0036\n",
      "Dandiset Web URL: https://dandiarchive.org/dandiset/001354/0.250312.0036\n",
      "Dandiset description: This dataset contains single cell electrophysiological recordings of mouse hippocampal CA1 neurons in response to activation of programmable antigen-gated G-protein-coupled engineered receptors. Recorded neurons were transfected with an AAV1/2-hSyn-a-mCherry-PAGER-Gi-P2A-mEGFP and responses were then recorded in response to DCZ (100 nM) or DCZ + soluble mCherry (1 uM) application.\n",
      "\n",
      "The authors are grateful to the St Jude Children’s Research Hospital Collaborative Research Consortium on GPCRs, the Chan Zuckerberg Biohub–San Francisco, Phil and Penny Knight Initiative for Brain Resilience (KIG-104), Stanford Cancer Institute, Wu Tsai Neurosciences Institute of Stanford University and the NIH (MH135934 to A.Y.T. and I.S., F32CA257159 to N.A.K., NS121106 to I.S.) for funding this work. R.T. was supported by the Life Sciences Research Foundation Fellowship (sponsored by Astellas Pharma) and JSPS Overseas Research Fellowship.\n"
     ]
    }
   ],
   "source": [
    "# Connect to DANDI archive\n",
    "client = DandiAPIClient()\n",
    "dandiset_id = \"001354\"\n",
    "dandiset_version = \"0.250312.0036\"\n",
    "dandiset = client.get_dandiset(dandiset_id, dandiset_version)\n",
    "\n",
    "# Print basic information about the Dandiset\n",
    "metadata = dandiset.get_raw_metadata()\n",
    "print(f\"Dandiset name: {metadata['name']}\")\n",
    "print(f\"Dandiset URL: {metadata['url']}\") # This should be the API URL\n",
    "print(f\"Dandiset Web URL: https://dandiarchive.org/dandiset/{dandiset_id}/{dandiset_version}\")\n",
    "print(f\"Dandiset description: {metadata.get('description', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a5584",
   "metadata": {},
   "source": [
    "### List Assets in the Dandiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1146378",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:05.677424Z",
     "iopub.status.busy": "2025-05-09T19:35:05.676994Z",
     "iopub.status.idle": "2025-05-09T19:35:06.046798Z",
     "shell.execute_reply": "2025-05-09T19:35:06.046121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 assets:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RemoteBlobAsset' object has no attribute 'asset_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFirst 5 assets:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m asset \u001b[38;5;129;01min\u001b[39;00m islice(assets, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43masset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masset_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masset\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Corrected to asset.asset_id\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.12/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RemoteBlobAsset' object has no attribute 'asset_id'"
     ]
    }
   ],
   "source": [
    "# List some assets in the Dandiset\n",
    "assets = dandiset.get_assets()\n",
    "print(\"\\nFirst 5 assets:\")\n",
    "for asset in islice(assets, 5):\n",
    "    print(f\"- {asset.path} (ID: {asset.asset_id}, Size: {asset.size} bytes)\") # Corrected to asset.asset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a17d79",
   "metadata": {},
   "source": [
    "## Loading an NWB file\n",
    "\n",
    "We will now load one of the NWB files from the Dandiset and examine its metadata. We'll choose the first asset listed for this demonstration.\n",
    "\n",
    "**Path of the NWB file being loaded:** `sub-PK-109/sub-PK-109_ses-20240717T150830_slice-2024-07-17-0001_cell-2024-07-17-0001_icephys.nwb`\n",
    "\n",
    "**Asset ID:** `8609ffee-a79e-498c-8dfa-da46cffef135`\n",
    "\n",
    "To load the NWB file, we first need its download URL. The DANDI client can provide S3 URLs for direct streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cde326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:06.048836Z",
     "iopub.status.busy": "2025-05-09T19:35:06.048641Z",
     "iopub.status.idle": "2025-05-09T19:35:06.110256Z",
     "shell.execute_reply": "2025-05-09T19:35:06.109521Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An unexpected error occurred while trying to get asset information or load NWB: 'RemoteBlobAsset' object has no attribute 'asset_id'\n"
     ]
    }
   ],
   "source": [
    "# Let's pick the first asset from the previous list (or re-fetch if necessary)\n",
    "# Ensure assets are fetched if this cell is run independently\n",
    "assets_iterator = dandiset.get_assets()\n",
    "try:\n",
    "    first_asset = next(assets_iterator)\n",
    "    print(f\"Selected asset: {first_asset.path} with ID: {first_asset.asset_id}\")\n",
    "\n",
    "    # Get the S3 download URL for streaming\n",
    "    # For published versions, we can often construct the URL directly or use asset.get_content_url()\n",
    "    # The asset.get_content_url(follow_redirects=True, strip_redirects=True) method is robust\n",
    "    # For simplicity here, and if streaming directly via pynwb with an S3 URL, this is a common pattern:\n",
    "    nwb_s3_url = first_asset.get_content_url(follow_redirects=True, strip_redirects=True)\n",
    "    if not nwb_s3_url: # Fallback if direct S3 URL is not immediately available or requires specific flags\n",
    "        # A more general way to get a download URL:\n",
    "        nwb_api_download_url = f\"https://api.dandiarchive.org/api/assets/{first_asset.asset_id}/download/\"\n",
    "        print(f\"Using API download URL: {nwb_api_download_url}\")\n",
    "        # pynwb can sometimes handle this URL directly if it redirects to an S3 URL,\n",
    "        # or we might need to resolve it. Let's assume pynwb handles remfile usage.\n",
    "        # For direct streaming, we usually need the underlying S3 URL.\n",
    "        # The `tools_cli.py` script constructs this:\n",
    "        # https://api.dandiarchive.org/api/assets/<ASSET_ID>/download/\n",
    "        # We will use this form for the Neurosift link and attempt to use it for pynwb.\n",
    "        # pynwb supports http/https URLs, especially if they point to S3 storage, using remfile.\n",
    "        file_url_for_pynwb = nwb_api_download_url\n",
    "    else:\n",
    "        print(f\"S3 URL for streaming: {nwb_s3_url}\")\n",
    "        file_url_for_pynwb = nwb_s3_url\n",
    "\n",
    "\n",
    "    print(f\"\\nAttempting to load NWB file from: {file_url_for_pynwb}\")\n",
    "    # Note: The `tools_cli.py nwb-file-info` command timed out.\n",
    "    # The following code attempts generic NWB loading. Specific data paths may need adjustment.\n",
    "    # For streaming, ensure 'remfile' is installed. pynwb uses it automatically for S3 URLs.\n",
    "    # Set environment variable for S3 anonymous access by h5py/remfile if needed (often not necessary for public DANDI)\n",
    "    os.environ['HDF5_USE_ROS3'] = '1' # For direct H5PY S3 driver, pynwb's remfile is usually preferred for http\n",
    "\n",
    "    # Using 'r' mode and `load_namespaces=True` is standard.\n",
    "    # For remote files, pynwb typically uses `driver='ros3'` implicitly if path is S3, or `remfile` for http(s)\n",
    "    # If `file_url_for_pynwb` is the api.dandiarchive.org URL, `remfile` should handle it.\n",
    "    try:\n",
    "        io = NWBHDF5IO(path=file_url_for_pynwb, mode='r', load_namespaces=True) # Removed driver='ros3' to let pynwb decide\n",
    "        nwbfile = io.read()\n",
    "        print(\"\\nNWB file loaded successfully.\")\n",
    "        print(\"NWB File Info:\")\n",
    "        print(f\"  Identifier: {nwbfile.identifier}\")\n",
    "        print(f\"  Session description: {nwbfile.session_description}\")\n",
    "        print(f\"  Session start time: {nwbfile.session_start_time}\")\n",
    "        print(f\"  Experimenter: {nwbfile.experimenter}\")\n",
    "        print(f\"  Institution: {nwbfile.institution}\")\n",
    "        print(f\"  Lab: {nwbfile.lab}\")\n",
    "        \n",
    "        # Do not display the full nwbfile object here to avoid excessive output.\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NWB file: {e}\")\n",
    "        print(\"This might be due to network issues, or the specific URL format not being directly streamable by pynwb without further tools.\")\n",
    "        print(\"You might need to ensure 'remfile' is installed and working correctly, or try downloading the file first.\")\n",
    "        nwbfile = None # Ensure nwbfile is None if loading fails\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"No assets found in the Dandiset to load.\")\n",
    "    nwbfile = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while trying to get asset information or load NWB: {e}\")\n",
    "    nwbfile = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a218d9",
   "metadata": {},
   "source": [
    "### Explore NWB file on Neurosift\n",
    "\n",
    "You can explore the NWB file interactively on Neurosift using the following link:\n",
    "\n",
    "[https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/8609ffee-a79e-498c-8dfa-da46cffef135/download/&dandisetId=001354&dandisetVersion=0.250312.0036](https://neurosift.app/nwb?url=https://api.dandiarchive.org/api/assets/8609ffee-a79e-498c-8dfa-da46cffef135/download/&dandisetId=001354&dandisetVersion=0.250312.0036)\n",
    "(Note: The Neurosift link uses the version `0.250312.0036` as specified in the setup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657efde",
   "metadata": {},
   "source": [
    "## Summarizing NWB File Contents\n",
    "\n",
    "If the NWB file was loaded successfully, we can inspect its contents.\n",
    "**Reminder:** The `tools_cli.py nwb-file-info` command, which provides detailed information about data paths within the NWB file, timed out. The following examples are general and will attempt to access common NWB data structures. If these paths do not exist in this specific file, an error will occur or no data will be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4628190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:06.112328Z",
     "iopub.status.busy": "2025-05-09T19:35:06.112123Z",
     "iopub.status.idle": "2025-05-09T19:35:06.119858Z",
     "shell.execute_reply": "2025-05-09T19:35:06.119315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWB file not loaded, skipping content summary.\n"
     ]
    }
   ],
   "source": [
    "if nwbfile:\n",
    "    print(\"Exploring NWB file structure:\")\n",
    "    \n",
    "    print(\"\\nAvailable acquisition objects (potential raw data):\")\n",
    "    if nwbfile.acquisition:\n",
    "        for name, series in nwbfile.acquisition.items():\n",
    "            print(f\"- {name} (type: {type(series).__name__})\")\n",
    "            if hasattr(series, 'data'):\n",
    "                 print(f\"  Shape: {series.data.shape}, Dtype: {series.data.dtype}\")\n",
    "            if hasattr(series, 'timestamps') and series.timestamps is not None:\n",
    "                 print(f\"  Timestamps length: {len(series.timestamps)}\")\n",
    "    else:\n",
    "        print(\"  No acquisition objects found.\")\n",
    "\n",
    "    print(\"\\nAvailable stimulus objects:\")\n",
    "    if nwbfile.stimulus:\n",
    "        for name, series in nwbfile.stimulus.items():\n",
    "            print(f\"- {name} (type: {type(series).__name__})\")\n",
    "            if hasattr(series, 'data'):\n",
    "                 print(f\"  Shape: {series.data.shape}, Dtype: {series.data.dtype}\")\n",
    "    else:\n",
    "        print(\"  No stimulus objects found.\")\n",
    "\n",
    "    print(\"\\nAvailable processing modules:\")\n",
    "    if nwbfile.processing:\n",
    "        for module_name, processing_module in nwbfile.processing.items():\n",
    "            print(f\"- Processing module: {module_name}\")\n",
    "            for data_interface_name, data_interface in processing_module.data_interfaces.items():\n",
    "                print(f\"  - {data_interface_name} (type: {type(data_interface).__name__})\")\n",
    "    else:\n",
    "        print(\"  No processing modules found.\")\n",
    "        \n",
    "    print(\"\\nAvailable icephys (intracellular electrophysiology) data:\")\n",
    "    # According to metadata: variableMeasured: [\"CurrentClampSeries\", \"CurrentClampStimulusSeries\"]\n",
    "    # These are typically found under `nwbfile.icephys` or in `nwbfile.acquisition`\n",
    "    if nwbfile.icephys:\n",
    "        print(\"Icephys metadata:\")\n",
    "        for series_name, series_data in nwbfile.icephys.items():\n",
    "            print(f\"- Sweep {series_name}: (type: {type(series_data).__name__})\")\n",
    "            # Example based on common CurrentClampSeries attributes\n",
    "            if hasattr(series_data, 'data') and series_data.data is not None:\n",
    "                print(f\"  Data shape: {series_data.data.shape}, Unit: {series_data.conversion} {series_data.unit}\")\n",
    "            if hasattr(series_data, 'stimulus_description'):\n",
    "                 print(f\"  Stimulus: {series_data.stimulus_description}\")\n",
    "            if hasattr(series_data, 'sweep_number'):\n",
    "                 print(f\"  Sweep Number: {series_data.sweep_number}\")\n",
    "    else:\n",
    "        # Check acquisition if not directly in icephys (less standard for processed icephys but possible)\n",
    "        found_icephys_in_acq = False\n",
    "        if nwbfile.acquisition:\n",
    "            for name, series in nwbfile.acquisition.items():\n",
    "                if isinstance(series, (pynwb.icephys.CurrentClampSeries, pynwb.icephys.VoltageClampSeries, pynwb.icephys.CurrentClampStimulusSeries, pynwb.icephys.VoltageClampStimulusSeries)):\n",
    "                    if not found_icephys_in_acq:\n",
    "                        print(\"Icephys series found in nwbfile.acquisition:\")\n",
    "                        found_icephys_in_acq = True\n",
    "                    print(f\"- {name} (type: {type(series).__name__})\")\n",
    "                    if hasattr(series, 'data'):\n",
    "                        print(f\"  Data shape: {series.data.shape}, Unit: {series.conversion} {series.unit if hasattr(series, 'unit') else 'N/A'}\")\n",
    "        if not found_icephys_in_acq:\n",
    "            print(\"  No dedicated icephys objects found in nwbfile.icephys or common icephys types in nwbfile.acquisition.\")\n",
    "\n",
    "else:\n",
    "    print(\"NWB file not loaded, skipping content summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68e16d",
   "metadata": {},
   "source": [
    "## Loading and Visualizing Data from the NWB File\n",
    "\n",
    "We will now attempt to load and visualize some data. As mentioned, specific paths for `CurrentClampSeries` or other data types are not confirmed due to the earlier timeout. We will try to access data typically found in `icephys` experiments.\n",
    "\n",
    "**Note on remote data access:** Accessing data from remote NWB files involves streaming. For large datasets, it's crucial to load only necessary subsets to avoid long loading times and high memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbf1f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:06.122408Z",
     "iopub.status.busy": "2025-05-09T19:35:06.122206Z",
     "iopub.status.idle": "2025-05-09T19:35:06.131107Z",
     "shell.execute_reply": "2025-05-09T19:35:06.130656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NWB file not loaded, skipping data visualization.\n"
     ]
    }
   ],
   "source": [
    "if nwbfile:\n",
    "    # Try to find CurrentClampSeries, often in acquisition or icephys\n",
    "    ccs_series = None\n",
    "    series_name_to_plot = None\n",
    "\n",
    "    # First check nwbfile.acquisition\n",
    "    if nwbfile.acquisition:\n",
    "        for name, series_obj in nwbfile.acquisition.items():\n",
    "            if isinstance(series_obj, pynwb.icephys.CurrentClampSeries):\n",
    "                ccs_series = series_obj\n",
    "                series_name_to_plot = f\"acquisition['{name}']\"\n",
    "                print(f\"Found CurrentClampSeries in acquisition: {name}\")\n",
    "                break\n",
    "    \n",
    "    # If not in acquisition, check nwbfile.icephys (less common for the series itself, but possible)\n",
    "    if not ccs_series and nwbfile.icephys:\n",
    "         for name, series_obj in nwbfile.icephys.items():\n",
    "            if isinstance(series_obj, pynwb.icephys.CurrentClampSeries):\n",
    "                ccs_series = series_obj\n",
    "                series_name_to_plot = f\"icephys['{name}']\"\n",
    "                print(f\"Found CurrentClampSeries in icephys: {name}\")\n",
    "                break\n",
    "    \n",
    "    if ccs_series:\n",
    "        print(f\"Plotting data from: {series_name_to_plot}\")\n",
    "        \n",
    "        data = ccs_series.data\n",
    "        timestamps = ccs_series.timestamps\n",
    "\n",
    "        # Determine how much data to plot to keep it manageable\n",
    "        num_points_to_plot = min(len(data), 2000) # Plot up to 2000 points\n",
    "        \n",
    "        # Load a subset of data and corresponding timestamps\n",
    "        # Using direct slicing on HDF5 dataset\n",
    "        data_subset = data[:num_points_to_plot]\n",
    "        if timestamps is not None and len(timestamps) == len(data): # Timestamps might be regularly sampled\n",
    "            time_subset = timestamps[:num_points_to_plot]\n",
    "            time_unit = \"s\" # Assuming timestamps are in seconds\n",
    "        elif hasattr(ccs_series, 'starting_time') and hasattr(ccs_series, 'rate'):\n",
    "            # Generate time axis if timestamps are not explicitly stored but rate and starting_time are\n",
    "            time_subset = ccs_series.starting_time + np.arange(num_points_to_plot) / ccs_series.rate\n",
    "            time_unit = \"s\"\n",
    "        else:\n",
    "            # Fallback to sample numbers if no time information\n",
    "            time_subset = np.arange(num_points_to_plot)\n",
    "            time_unit = \"samples\"\n",
    "\n",
    "        print(f\"Plotting the first {num_points_to_plot} data points.\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        # Seaborn styling is already applied globally\n",
    "        plt.plot(time_subset, data_subset)\n",
    "        plt.title(f\"Example Current Clamp Data ({series_name_to_plot}) - First {num_points_to_plot} points\")\n",
    "        plt.xlabel(f\"Time ({time_unit})\")\n",
    "        plt.ylabel(f\"Voltage ({ccs_series.conversion if ccs_series.conversion else 1.0} {ccs_series.unit})\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Also try to plot stimulus if available\n",
    "        # Let's assume stimulus would be a CurrentClampStimulusSeries\n",
    "        # and try to find one, perhaps related by name or structure\n",
    "        stim_series = None\n",
    "        if nwbfile.stimulus:\n",
    "            for name, series_obj in nwbfile.stimulus.items():\n",
    "                if isinstance(series_obj, pynwb.icephys.CurrentClampStimulusSeries):\n",
    "                    # Heuristic: match by name if possible, or just take the first one\n",
    "                    # This is highly speculative without `nwb-file-info`\n",
    "                    stim_series = series_obj\n",
    "                    print(f\"Found CurrentClampStimulusSeries in stimulus: {name}\")\n",
    "                    break # Take the first one for now\n",
    "        \n",
    "        if stim_series:\n",
    "            stim_data = stim_series.data\n",
    "            stim_num_points_to_plot = min(len(stim_data), num_points_to_plot) # Match length if possible\n",
    "            stim_data_subset = stim_data[:stim_num_points_to_plot]\n",
    "            \n",
    "            if hasattr(stim_series, 'starting_time') and hasattr(stim_series, 'rate'):\n",
    "                stim_time_subset = stim_series.starting_time + np.arange(stim_num_points_to_plot) / stim_series.rate\n",
    "                stim_time_unit = \"s\"\n",
    "            else:\n",
    "                stim_time_subset = np.arange(stim_num_points_to_plot)\n",
    "                stim_time_unit = \"samples\"\n",
    "\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(stim_time_subset, stim_data_subset, color='orange')\n",
    "            plt.title(f\"Example Stimulus Data ({name}) - First {stim_num_points_to_plot} points\")\n",
    "            plt.xlabel(f\"Time ({stim_time_unit})\")\n",
    "            plt.ylabel(f\"Current ({stim_series.conversion if stim_series.conversion else 1.0} {stim_series.unit})\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "            \n",
    "    else:\n",
    "        print(\"Could not find a suitable CurrentClampSeries to plot.\")\n",
    "        print(\"This might be because the NWB file structure is different than assumed,\")\n",
    "        print(\"or the data is not present under common paths like 'acquisition' or 'icephys'.\")\n",
    "        print(\"Consult the Neurosift link or use H5Py to explore the NWB file structure manually.\")\n",
    "\n",
    "else:\n",
    "    print(\"NWB file not loaded, skipping data visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a5e0c",
   "metadata": {},
   "source": [
    "## Summary and Future Directions\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "- Connect to the DANDI archive and retrieve information about Dandiset 001354.\n",
    "- List assets within the Dandiset.\n",
    "- Attempt to load an NWB file from the Dandiset for analysis, along with a link to explore it on Neurosift.\n",
    "- Show a generic example of how to access and plot time series data (CurrentClampSeries) that might be present in such NWB files.\n",
    "\n",
    "**Limitations & Challenges:**\n",
    "- The `tools_cli.py nwb-file-info` command, which is crucial for understanding the detailed internal structure and specific data paths within NWB files, timed out during the preparation of this notebook. This means that the data loading and visualization examples are based on common NWB conventions (e.g., for CurrentClampSeries) and may not perfectly match the structure of the files in this specific Dandiset. Users may need to use tools like H5Py or Neurosift to identify the exact paths to their data of interest.\n",
    "- Visualizing raw electrophysiology data often requires careful selection of time ranges and potentially downsampling or averaging, especially for remote files, to manage performance.\n",
    "\n",
    "**Possible Future Directions:**\n",
    "1.  **Detailed Exploration:** Use H5Py (e.g., `h5py.File(nwb_file_url, 'r', driver='ros3')`) or the Neurosift link to thoroughly explore the NWB file structure and identify all relevant data groups and datasets (e.g., specific sweep numbers, stimulus protocols, cell metadata).\n",
    "2.  **Targeted Analysis:** Once specific data paths are known, perform targeted analyses. For example:\n",
    "    *   Compare neuronal responses across different experimental conditions (e.g., DCZ vs. DCZ + mCherry application) if stimulus information is clearly segregated.\n",
    "    *   Extract features from responses, such as spike times (if applicable and sufficiently clear in current clamp), action potential shapes, or changes in membrane potential.\n",
    "    *   Analyze data from multiple cells or multiple subjects if the Dandiset contains such data.\n",
    "3.  **Advanced Visualizations:** Create more sophisticated visualizations, such as:\n",
    "    *   Overlaying responses from different sweeps or conditions.\n",
    "    *   Plotting I-V curves if data from multiple current injection steps are available.\n",
    "    *   Creating heatmaps of activity across many trials or cells.\n",
    "4.  **Integrate with Other Tools:** Utilize other Python libraries for neurophysiology data analysis (e.g., `spikeinterface` for spike sorting if extracellular data were present, or custom analysis scripts) to process the data further.\n",
    "\n",
    "Users are encouraged to adapt the code provided, especially the data access parts, based on the actual structure of the NWB files in this Dandiset. Exploring documentation associated with the Dandiset or the NWB standard can also provide valuable context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a493b641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T19:35:06.133051Z",
     "iopub.status.busy": "2025-05-09T19:35:06.132623Z",
     "iopub.status.idle": "2025-05-09T19:35:06.135920Z",
     "shell.execute_reply": "2025-05-09T19:35:06.135242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Final cleanup if an NWB file was opened (optional, Python's garbage collector usually handles it)\n",
    "if 'io' in locals() and io is not None:\n",
    "    try:\n",
    "        io.close()\n",
    "        print(\"NWB IO closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing NWB IO: {e}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
